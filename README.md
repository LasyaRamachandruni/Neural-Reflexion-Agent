# ğŸ§  Neural Reflexion Agent

**Self-Improving AI Reasoning with LangGraph, Gemini, Tavily & Streamlit**

## ğŸš€ Overview

The Neural Reflexion Agent simulates human-like reasoning, critique, and self-improvement. It uses LangGraph to model a feedback-driven cognitive loop where an LLM (Gemini) drafts, reflects, retrieves evidence, and revises its own answers using real-time web data (Tavily).

The Streamlit UI lets you interactively explore each reasoning cycle, view citations, compare runs, and export results.

## ğŸ§© Key Features

- âœ… **LangGraph Reflexion Loop** â€” iterative reasoning chain: draft â†’ execute_tools â†’ revisor
- âœ… **Self-Critique & Revision** â€” each iteration improves clarity, evidence, and structure
- âœ… **Live Web Search** â€” integrates the Tavily API for real-time data grounding
- âœ… **Heuristic Reward Scoring** â€” mimics RL convergence with a self-evaluation score
- âœ… **Streamlit UI** â€” intuitive front-end for prompts, iterations, results, and comparisons
- âœ… **Downloadable Outputs** â€” export final answer (Markdown) or full reasoning trace (JSON)

## ğŸ§  System Architecture

```mermaid
graph TD
    A[Draft Gemini] --> B[Execute Tools Tavily]
    B --> C[Revisor Gemini]
    C -->|Improvement Needed| B
    C -->|Converged| D[End]
```

1. **Draft** â†’ Generates initial answer & self-reflection
2. **Execute Tools** â†’ Runs Tavily search for suggested queries
3. **Revisor** â†’ Refines answer using new evidence and adds citations
4. **Loop Control** â†’ Stops automatically when reward score stabilizes

## ğŸ§® Reward-Driven Reflexion

Each revision is evaluated with a heuristic reward function that scores:

- Conciseness (~250 words target)
- Number of valid references
- Inline citations ([1], [2])
- Query coverage
- Iterative improvement

When the reward stops increasing â†’ the loop ends.

This creates a reinforcement-inspired reasoning process â€” language-based self-improvement without gradient updates.

## ğŸ” Relation to Reinforcement Learning

This project doesn't perform gradient-based RL (no Q-learning or PPO). However, it borrows the conceptual structure of reinforcement learning:

| RL Concept | Reflexion Equivalent |
|------------|---------------------|
| Environment | LangGraph + Tavily + LLM |
| State | Conversation + evidence context |
| Action | Revised answer |
| Reward | Heuristic self-evaluation score |
| Policy Update | Prompt-level behavior change via self-reflection |

This approach, often called **language-based self-reinforcement**, shows how LLMs can simulate RL-like learning through reflection and scoring rather than model fine-tuning.

## ğŸ§° Project Structure

```
neural-reflexion-agent/
â”œâ”€â”€ chains.py              # Defines Gemini prompt chains (draft & revisor)
â”œâ”€â”€ execute_tools.py        # Tavily search tool executor
â”œâ”€â”€ reflexion_agent.py      # Core LangGraph pipeline + scoring logic
â”œâ”€â”€ schema.py               # Pydantic tool models
â”œâ”€â”€ ui_app.py               # Streamlit interface
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ .env                    # API keys (not committed)
```

## ğŸ’» Streamlit UI

Run:

```bash
streamlit run ui_app.py
```

### UI Highlights:

- ğŸ§  Prompt box + "Run Reflexion" button
- âš™ï¸ Sidebar controls (max iterations, environment key checks)
- ğŸ” Live run status
- ğŸ“„ Final answer with citations
- ğŸŒ Deduplicated sources from Tavily
- ğŸ“Š Run history + side-by-side comparison
- ğŸ’¾ Export to Markdown or JSON

## ğŸ“¦ Installation

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Add your API keys in `.env`

```env
GOOGLE_API_KEY=your_gemini_key_here
TAVILY_API_KEY=your_tavily_key_here
```

### 3ï¸âƒ£ Run the agent in CLI mode

```bash
python reflexion_agent.py
```

### 4ï¸âƒ£ Launch the Streamlit UI

```bash
streamlit run ui_app.py
```

Then open `http://localhost:8501`.

## ğŸ“˜ Example Output

**Prompt:**

> Write about how small business can leverage AI to grow.

**Final Answer:**

> Small businesses can strategically leverage artificial intelligence (AI) to accelerate growth without significant investment. In marketing, platforms like Mailchimp use AI to optimize email subject lines, improving open rates [1]. Chatbots such as Tidio can automate up to 87% of routine queries, cutting costs [2].
>
> Operationally, AI tools like QuickBooks automate invoicing and expense tracking, reducing administrative overhead by 29% [3]. Small businesses should start with affordable, scalable tools requiring minimal technical expertise.
>
> Ethical adoption is crucial: companies must ensure transparency and compliance with GDPR and CCPA [4]. By focusing on specific, measurable areas and responsible practices, small businesses can use AI as a sustainable growth engine.

**References:**

- [1] Mailchimp Subject Line Helper
- [2] Tidio Chatbot Statistics
- [3] McKinsey: State of AI 2023
- [4] Forbes Tech Council: AI & Data Privacy

## âš™ï¸ Dependencies

```
langchain>=0.2.16
langgraph>=0.2.27
langchain-google-genai>=2.0.4
langchain-tavily>=0.1.0
tavily-python>=0.5.0
pydantic>=2.7.0
python-dotenv
google-generativeai
google-ai-generativelanguage
streamlit
```

## ğŸ§© Future Extensions

| Feature | Description |
|---------|-------------|
| ğŸ§  Persistent Memory | Store learned reflections and source history across sessions |
| ğŸ“Š Reward Visualization | Track reward scores across iterations |
| ğŸ’¬ Interactive Chat Mode | Let users refine prompts conversationally |
| ğŸ§¾ Q-Learning Extension | Apply numeric rewards and gradient-free policy updates |
| ğŸŒ Multi-Agent Reflexion | Cooperative agents that review each other's work |

## ğŸ“ Educational Value

This project bridges:

- **Cognitive Science** (self-reflection)
- **Reinforcement Learning Theory**
- **Retrieval-Augmented Generation**
- **LLM-Orchestrated Reasoning Graphs**

It's an ideal foundation for research or a showcase of AI self-improvement mechanisms â€” combining LangGraph, Google Gemini, and Tavily into a single, interpretable reasoning framework.

## ğŸ§¾ License

MIT License Â© 2025

You are free to modify and distribute this project with attribution.


